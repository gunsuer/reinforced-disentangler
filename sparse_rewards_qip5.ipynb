{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyclifford as pc\n",
    "import gymnasium as gym\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclifford as pc\n",
    "from pyclifford.utils import mask, condense, pauli_diagonalize1,stabilizer_measure\n",
    "from pyclifford.paulialg import Pauli, pauli, PauliMonomial, pauli_zero\n",
    "from pyclifford.stabilizer import (StabilizerState, CliffordMap,\n",
    "    zero_state, identity_map, clifford_rotation_map, random_clifford_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layers(N_QUBITS,HALF_DEPTH):\n",
    "    random_layers=[]\n",
    "    for i in range (int(HALF_DEPTH)):\n",
    "        random_layer=[]\n",
    "        if i%2==0:\n",
    "            for j in range (int(np.floor(N_QUBITS/2))):\n",
    "                gate=pc.CliffordGate(j*2,j*2+1)\n",
    "                gate.set_forward_map(pc.random_clifford_map(2))\n",
    "                random_layer.append(gate)\n",
    "            random_layers.append(random_layer)\n",
    "        elif i%2==1:\n",
    "            for j in range (int(np.floor(N_QUBITS/2))):\n",
    "                gate=pc.CliffordGate(j*2+1,(j*2+2)%N_QUBITS)\n",
    "                gate.set_forward_map(pc.random_clifford_map(2))\n",
    "                random_layer.append(gate)\n",
    "            random_layers.append(random_layer)\n",
    "    return random_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_layers(N_QUBITS,HALF_DEPTH,theta):\n",
    "    measure_layers=[]\n",
    "    for i in range (int(HALF_DEPTH)):\n",
    "        measure_layer=[]\n",
    "        for j in range (int(N_QUBITS)):\n",
    "            if theta[-i + HALF_DEPTH - 1][j]==1:\n",
    "                measure_layer.append(j)\n",
    "        measure_layers.append(measure_layer)\n",
    "    return measure_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circuit(N_QUBITS,HALF_DEPTH,random_layers,measure_layers):\n",
    "    circ = pc.circuit.Circuit(N_QUBITS)\n",
    "    for i in range(int(HALF_DEPTH)):\n",
    "        for j in range(int(np.floor(N_QUBITS/2))):\n",
    "            circ.take(random_layers[i][j])\n",
    "        if measure_layers[i]!=[]:\n",
    "            qubits=tuple(measure_layers[i])\n",
    "            circ.measure(*qubits)\n",
    "            \n",
    "    return circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_EE(state_final):\n",
    "    EE_positions=[]\n",
    "    for i in range(0,state_final.N):\n",
    "        EE_positions.append([int(i),int(i+1)%state_final.N])\n",
    "        \n",
    "    EE_list=[]\n",
    "    for i in range(0,len(EE_positions)):\n",
    "        EE_list.append(state_final.entropy(EE_positions[i]))\n",
    "    \n",
    "    averaged_EE=np.mean(EE_list)\n",
    "        \n",
    "    return averaged_EE #, EE_positions, EE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(x,penalty_slope):\n",
    "    return 2*(0.5-(1/(1+np.exp(-penalty_slope*x))-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disentangler(gym.Env):\n",
    "    \"\"\"\n",
    "    Reinforcement learning environment for the disentangler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits, half_depth, positive_reward, penalty_slope):\n",
    "        super(Disentangler, self).__init__()\n",
    "        \n",
    "        self.N_QUBITS = n_qubits\n",
    "        self.HALF_DEPTH = half_depth\n",
    "        self.DEPTH = 2*half_depth\n",
    "        self.positive_reward = positive_reward\n",
    "        self.penalty_slope = penalty_slope\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(self.N_QUBITS * self.HALF_DEPTH)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.HALF_DEPTH, self.N_QUBITS), dtype=np.int8)\n",
    "\n",
    "        self.random_layers=random_layers(self.N_QUBITS,self.HALF_DEPTH)\n",
    "        self.theta = np.zeros((self.HALF_DEPTH, self.N_QUBITS), dtype=np.int8)\n",
    "        self.measure_layers=measure_layers(self.N_QUBITS,self.HALF_DEPTH,self.theta)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Initialize reward and done\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncate = False\n",
    "\n",
    "        # Apply the action\n",
    "        h = np.zeros(self.N_QUBITS * self.HALF_DEPTH, dtype=np.int8)\n",
    "        h[action] = 1\n",
    "        h = h.reshape((self.HALF_DEPTH, self.N_QUBITS))\n",
    "        self.theta = (self.theta + h) % 2\n",
    "\n",
    "        # Calculate entropy (assumes circuit is a predefined function)\n",
    "        self.measure_layers=measure_layers(self.N_QUBITS, self.HALF_DEPTH, self.theta)\n",
    "        circ = create_circuit(self.N_QUBITS, self.HALF_DEPTH, self.random_layers, self.measure_layers)\n",
    "\n",
    "        state_initial = pc.stabilizer.zero_state(self.N_QUBITS)\n",
    "        state_final = circ.forward(state_initial)\n",
    "\n",
    "        entropy = averaged_EE(state_final)\n",
    "\n",
    "        \n",
    "        if entropy == 0:\n",
    "            m_per_layer = [np.sum(layer) for layer in self.theta]\n",
    "            cost = [m_per_layer[i]*penalty(i,self.penalty_slope) for i in range(self.HALF_DEPTH)]\n",
    "            reward = self.positive_reward - np.sum(cost)#/(self.N_QUBITS*self.DEPTH)\n",
    "            done = True\n",
    "        \n",
    "        # Return the state, reward, done flag, truncate flag, and info\n",
    "        info = {}\n",
    "        return self.theta, reward, done, truncate, info\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # Seed the random number generator if a seed is provided\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Reset the state to an all-zero matrix\n",
    "        self.theta = np.zeros((self.HALF_DEPTH, self.N_QUBITS), dtype=np.int8)\n",
    "\n",
    "        info = {}\n",
    "        return self.theta, info\n",
    "    \n",
    "    def render(self):\n",
    "        print()\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Implement any cleanup\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo parameters\n",
    "ts = 1e6\n",
    "lr = 0.001\n",
    "ec = 0.02\n",
    "\n",
    "# circuit parameters\n",
    "N_QUBITS=10\n",
    "HALF_DEPTH=5\n",
    "DEPTH=int(2*HALF_DEPTH)\n",
    "positive_reward = 50\n",
    "penalty_slope = 0\n",
    "\n",
    "env = Disentangler(N_QUBITS,HALF_DEPTH,positive_reward,penalty_slope)\n",
    "\n",
    "env.reset()\n",
    "model = PPO('MlpPolicy', env, verbose=0, tensorboard_log=\"./tensorboard_files\", learning_rate=lr, ent_coef=ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=ts, tb_log_name=\"{N_QUBITS}x{HALF_DEPTH}_tb_{ts}_pr_{positive_reward}_ps_{penalty_slope}\".format(N_QUBITS=N_QUBITS, HALF_DEPTH=HALF_DEPTH, ts=ts, positive_reward=positive_reward, penalty_slope=penalty_slope))\n",
    "model_path = \"./models/{N_QUBITS}x{HALF_DEPTH}_tb_{ts}_pr_{positive_reward}_ps_{penalty_slope}\".format(N_QUBITS=N_QUBITS, HALF_DEPTH=HALF_DEPTH, ts=ts, positive_reward=positive_reward, penalty_slope=penalty_slope)\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(model_path)\n",
    "\n",
    "episodes = 100\n",
    "measurements = []\n",
    "for _ in range(episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action, state = model.predict(obs)\n",
    "        obs, rewards, done, truncate, info = env.step(action)\n",
    "\n",
    "    measurements.append(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "averaged_entropies = []\n",
    "for measurement in measurements:\n",
    "    entropies = []\n",
    "    for _ in range(episodes):\n",
    "        ran_lay=random_layers(N_QUBITS,HALF_DEPTH)\n",
    "        mea_lay=measure_layers(N_QUBITS,HALF_DEPTH,measurement)\n",
    "        circ = create_circuit(N_QUBITS,HALF_DEPTH,ran_lay,mea_lay)\n",
    "        state_initial = pc.stabilizer.zero_state(N_QUBITS)\n",
    "        state_final = circ.forward(state_initial)\n",
    "        entropy = averaged_EE(state_final)\n",
    "        entropies.append(entropy)\n",
    "    \n",
    "    averaged_entropies.append(np.mean(entropies))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(averaged_entropies)\n",
    "plt.xlabel('Averaged von Neumann Entropy')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Averaged von Neumann Entropies')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "averaged_measurements = np.mean(measurements, axis=0)\n",
    "variance_measurements = np.var(measurements, axis=0)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(averaged_measurements, cmap=\"YlGnBu\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(variance_measurements, cmap=\"hot\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "averaged_measurements = np.mean(measurements, axis=0)\n",
    "variance_measurements = np.var(measurements, axis=0)\n",
    "\n",
    "cord=[[i,j] for i in range (HALF_DEPTH) for j in range (N_QUBITS)]\n",
    "\n",
    "\n",
    "y=np.transpose([cord[i][0] for i in range (N_QUBITS*HALF_DEPTH)])\n",
    "x=np.transpose([cord[i][1] for i in range (N_QUBITS*HALF_DEPTH)])\n",
    "z=np.zeros(N_QUBITS*HALF_DEPTH,int)\n",
    "\n",
    "width = depth = 0.5\n",
    "\n",
    "avg_m=[]\n",
    "for i in range (HALF_DEPTH-1,-1,-1):\n",
    "    avg_m.append(averaged_measurements[i])\n",
    "heights=np.concatenate(avg_m)\n",
    "\n",
    "\n",
    "#plt.xlim(max(x), min(x))\n",
    "\n",
    "ax.bar3d(x-0.2, y-0.4, z, width, depth, heights, shade=True)\n",
    "ax.set_box_aspect([int(N_QUBITS/HALF_DEPTH), 1, 1]) \n",
    "ax.set_xticks([i for i in range (N_QUBITS)]) \n",
    "ax.set_yticks([i for i in range (HALF_DEPTH)]) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0,0.5,1,4,5]\n",
    "for aa in alpha:\n",
    "    x = range(DEPTH)\n",
    "    y = [penalty(xx,aa) for xx in x]\n",
    "    plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_star = measurements[0]\n",
    "theta_star"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
