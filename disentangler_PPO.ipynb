{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cliff2():\n",
    "    \"\"\"\n",
    "    Random 2-qubit Clifford circuit.\n",
    "\n",
    "    Arguments:\n",
    "        -nodes (np.ndarray): \n",
    "    \n",
    "    Returns:\n",
    "        -null\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = np.random.randint(2, size=(2, 10))\n",
    "    \n",
    "    return qml.matrix(qml.RandomLayers(weights=weights,wires=[0,1])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomLayers(N_QUBITS, DEPTH):\n",
    "    \"\"\"\n",
    "    Generates brick wall pattern of random 2 qubit Clifford gates\n",
    "\n",
    "    Arguments:\n",
    "        -N_QUBITS (int): Number of qubits\n",
    "        -DEPTH (int): Depth of the circuit\n",
    "\n",
    "    Returns:\n",
    "        -random_layers (np.ndarray): Array of 4x4 unitaries (N_QUBITS, DEPTH, 4, 4)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    random_layers = []\n",
    "    for t in range(DEPTH):\n",
    "        layer = []\n",
    "        for x in range(0,N_QUBITS,2):\n",
    "                layer.append(Cliff2())\n",
    "        random_layers.append(layer)\n",
    "\n",
    "    return random_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUBITS = 2*3\n",
    "DEPTH = 2\n",
    "\n",
    "# random_layers = []\n",
    "# # for t in range(DEPTH):\n",
    "# #         layer = []\n",
    "# #         for x in range(0,N_QUBITS,2):\n",
    "# #                 layer.append(Cliff2())\n",
    "# #         random_layers.append(layer)\n",
    "\n",
    "random_layers = RandomLayers(N_QUBITS,DEPTH)\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(theta):\n",
    "    \"\"\"\n",
    "    Quantum circuit with random entangling Clifford layers and disentangling layers.\n",
    "    \n",
    "    Arguments:\n",
    "        -theta (np.ndarray): Binary matrix representing the positions of projections. (N_QUBITS, DEPTH)\n",
    "    \n",
    "    Returns:\n",
    "        -Average Von Neumann entropy (float32): Average of 2-qubit Von Neumann entropies over all neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    theta = theta.T\n",
    "    DEPTH,N_QUBITS = np.shape(theta)\n",
    "\n",
    "    for t in range(DEPTH):\n",
    "        layer = random_layers[t]\n",
    "        if t%2==0:\n",
    "            for x in range(0,N_QUBITS,2):\n",
    "                brick = layer[int(x/2)]\n",
    "                qml.QubitUnitary(brick,wires=[x,x+1])\n",
    "        elif t%2==1:\n",
    "            for x in range(1,N_QUBITS-2,2):\n",
    "                brick = layer[int((x-1)/2)]\n",
    "                qml.QubitUnitary(brick,wires=[x,x+1])\n",
    "            brick = layer[-1]\n",
    "            qml.QubitUnitary(brick,wires=[N_QUBITS-1,0])\n",
    "            \n",
    "        projections = theta[t]\n",
    "        for x in range(N_QUBITS):\n",
    "            if projections[x]==1:\n",
    "                qml.Projector(state=[0],wires=[x])\n",
    "            \n",
    "    entropies = []\n",
    "    for x in range(N_QUBITS-1):\n",
    "        entropies.append(qml.vn_entropy(wires=[x,x+1]))\n",
    "    entropies.append(qml.vn_entropy(wires=[N_QUBITS-1,0]))\n",
    "        \n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.97490862e-01-2.48823376e-02j, -1.38777878e-17-4.45242875e-01j,\n",
       "        -3.54012799e-01+5.67876398e-01j,  2.85887162e-01+1.55249344e-01j],\n",
       "       [ 2.90777382e-01+1.54465493e-01j,  2.85887162e-01-6.09898057e-01j,\n",
       "         3.71203570e-01-2.40565752e-01j,  2.91926582e-01+3.96228110e-01j],\n",
       "       [ 9.11822553e-02-6.62943785e-01j,  1.55249344e-01+2.85887162e-01j,\n",
       "         3.98294357e-01+2.99128557e-01j,  4.45242875e-01+1.38777878e-17j],\n",
       "       [-2.40565752e-01+3.71203570e-01j,  2.90960383e-02+4.91295496e-01j,\n",
       "        -1.54465493e-01-2.90777382e-01j,  6.09898057e-01+2.85887162e-01j]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_layers[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06723857332377185, 1.4539090546957973e-15, 0.2971991799364353, 4.696891297018427e-15, 0.22996060661266332, 1.6236596776405806e-15]\n",
      "<class 'list'>\n",
      "0: ─╭U(M0)────────────────╭U(M5)──|0⟩⟨0|─┤ ╭vnentropy                                            \n",
      "1: ─╰U(M0)─────────╭U(M3)─│──────────────┤ ╰vnentropy ╭vnentropy                                 \n",
      "2: ─╭U(M1)──|0⟩⟨0|─╰U(M3)─│──────────────┤            ╰vnentropy ╭vnentropy                      \n",
      "3: ─╰U(M1)──|0⟩⟨0|─╭U(M4)─│──────────────┤                       ╰vnentropy ╭vnentropy           \n",
      "4: ─╭U(M2)─────────╰U(M4)─│──────────────┤                                  ╰vnentropy ╭vnentropy\n",
      "5: ─╰U(M2)──|0⟩⟨0|────────╰U(M5)─────────┤                                             ╰vnentropy\n",
      "\n",
      "  ╭vnentropy\n",
      "  │         \n",
      "  │         \n",
      "  │         \n",
      "  │         \n",
      "  ╰vnentropy\n",
      "\n",
      "M0 = \n",
      "[[-0.41310137+0.10651587j  0.2179702 +0.33404486j -0.75994442+0.18379512j\n",
      "  -0.11647631-0.18450404j]\n",
      " [-0.12048319-0.1814011j   0.47829843-0.61791042j -0.21578124-0.33574001j\n",
      "  -0.2666944 +0.33404486j]\n",
      " [-0.48284135-0.61494749j -0.11647631+0.18450404j  0.25837864+0.33946848j\n",
      "  -0.2179702 +0.33404486j]\n",
      " [ 0.21578124-0.33574001j  0.41627156+0.0971075j  -0.12048319+0.1814011j\n",
      "   0.75821254+0.18893494j]]\n",
      "M1 = \n",
      "[[-4.97490862e-01-2.48823376e-02j -1.38777878e-17-4.45242875e-01j\n",
      "  -3.54012799e-01+5.67876398e-01j  2.85887162e-01+1.55249344e-01j]\n",
      " [ 2.90777382e-01+1.54465493e-01j  2.85887162e-01-6.09898057e-01j\n",
      "   3.71203570e-01-2.40565752e-01j  2.91926582e-01+3.96228110e-01j]\n",
      " [ 9.11822553e-02-6.62943785e-01j  1.55249344e-01+2.85887162e-01j\n",
      "   3.98294357e-01+2.99128557e-01j  4.45242875e-01+1.38777878e-17j]\n",
      " [-2.40565752e-01+3.71203570e-01j  2.90960383e-02+4.91295496e-01j\n",
      "  -1.54465493e-01-2.90777382e-01j  6.09898057e-01+2.85887162e-01j]]\n",
      "M2 = \n",
      "[[-0.00550315-7.71765820e-01j -0.23033074+2.97429727e-01j\n",
      "  -0.41772911+8.82096466e-04j -0.29490524-3.69196029e-02j]\n",
      " [-0.00300638-4.21617589e-01j  0.42161759-5.44441463e-01j\n",
      "  -0.22820645+4.81891496e-04j  0.53982041+6.75808797e-02j]\n",
      " [ 0.22644226-3.51030327e-01j -0.19040475-2.28206453e-01j\n",
      "   0.64644518+4.21617589e-01j -0.12583026+3.54518601e-01j]\n",
      " [ 0.12370597-1.91768742e-01j  0.34853356+4.17729111e-01j\n",
      "   0.35315461+2.30330739e-01j  0.23033074-6.48941946e-01j]]\n",
      "M3 = \n",
      "[[-0.6091905 +0.33257438j  0.27349668-0.31127133j  0.02217984+0.53660081j\n",
      "   0.19000986+0.14850867j]\n",
      " [ 0.09206719+0.45658755j  0.24709225+0.66411157j  0.10645012-0.0396063j\n",
      "  -0.07290938+0.5125784j ]\n",
      " [-0.39160312-0.36753145j -0.011807  +0.24087169j  0.6498866 -0.24365177j\n",
      "   0.41434028-0.00352245j]\n",
      " [ 0.0396063 +0.10645012j -0.34026386+0.3902216j  -0.40572573+0.22876886j\n",
      "   0.66411157-0.24709225j]]\n",
      "M4 = \n",
      "[[-0.16610368-0.64356685j  0.19248713-0.12856606j -0.6336502 +0.04324965j\n",
      "  -0.0953385 -0.30360866j]\n",
      " [-0.00302496+0.1578549j   0.62152767+0.04405174j -0.28936607+0.21491424j\n",
      "   0.38371816+0.5564845j ]\n",
      " [ 0.01529843-0.67440638j -0.0604454 -0.2143341j   0.45224809-0.01241012j\n",
      "  -0.03239628+0.53816111j]\n",
      " [-0.20906662-0.18584162j  0.52595873+0.48161908j  0.49540772+0.12878229j\n",
      "   0.03107743-0.38753713j]]\n",
      "M5 = \n",
      "[[ 0.07458659+0.00942066j -0.28922952-0.12653281j -0.24949976+0.88282743j\n",
      "   0.15452962-0.17079235j]\n",
      " [ 0.25763088-0.55904846j  0.4388048 -0.18450097j  0.028236  +0.25638417j\n",
      "  -0.30004429+0.48779524j]\n",
      " [-0.46982627-0.38501328j -0.43769038+0.33559057j -0.27725061-0.03230466j\n",
      "  -0.49581341+0.05560318j]\n",
      " [ 0.39217155-0.30441543j -0.26250867+0.54830719j -0.06640359-0.09754264j\n",
      "   0.5420738 +0.27606727j]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randint(2, size=(N_QUBITS,DEPTH))\n",
    "print(circuit(theta))\n",
    "print(type(circuit(theta)))\n",
    "drawer = qml.draw(circuit)\n",
    "\n",
    "print(drawer(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disentangler(gym.Env):\n",
    "    \"\"\"\n",
    "    Reinforcement learning environment for the disentangler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits, depth):\n",
    "        super(Disentangler, self).__init__()\n",
    "        \n",
    "        self.N_QUBITS = n_qubits\n",
    "        self.DEPTH = depth\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(self.N_QUBITS * self.DEPTH)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.N_QUBITS, self.DEPTH), dtype=np.int8)\n",
    "        self.state = np.zeros((self.N_QUBITS, self.DEPTH), dtype=np.int8)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Initialize reward and done\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncate = False\n",
    "\n",
    "        # Apply the action\n",
    "        h = np.zeros(self.N_QUBITS * self.DEPTH, dtype=np.int8)\n",
    "        h[action] = 1\n",
    "        h = h.reshape((self.N_QUBITS, self.DEPTH))\n",
    "        self.state = (self.state + h) % 2\n",
    "\n",
    "        # Calculate entropy (assumes circuit is a predefined function)\n",
    "        entropies = circuit(self.state)\n",
    "        entropy = np.mean(entropies)\n",
    "\n",
    "        # Check if the state is trivial\n",
    "        trivial1 = (np.sum(self.state[:, -1]) == self.N_QUBITS)\n",
    "        trivial2 = (np.sum(self.state[:, -1]) == self.N_QUBITS - 1)\n",
    "        trivial = trivial1 or trivial2\n",
    "\n",
    "        # Determine reward and done conditions\n",
    "        if entropy < 1e-17:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        elif trivial:\n",
    "            reward = -1000\n",
    "            truncate = True\n",
    "        \n",
    "        # Return the state, reward, done flag, and info\n",
    "        info = {}\n",
    "        return self.state, reward, done, truncate, info\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # Seed the random number generator if a seed is provided\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Reset the state to an all-zero matrix\n",
    "        self.state = np.zeros((self.N_QUBITS, self.DEPTH), dtype=np.int8)\n",
    "\n",
    "        info = {}\n",
    "        return self.state, info\n",
    "    \n",
    "    def render(self):\n",
    "        print()\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Implement any cleanup\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Disentangler(n_qubits=N_QUBITS,depth=DEPTH)\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=10)\n",
    "env = Monitor(env, allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: \n",
      " (tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=int8, requires_grad=True), {})\n",
      "Observation: \n",
      " [[0 0]\n",
      " [0 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0, Done: False, Truncate: False, Info: {}\n"
     ]
    }
   ],
   "source": [
    "# Testing the environment\n",
    "env = Disentangler(n_qubits=N_QUBITS, depth=DEPTH)\n",
    "obs = env.reset()\n",
    "print(f\"Initial Observation: \\n {obs}\")\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, truncate, info = env.step(action)\n",
    "print(f\"Observation: \\n {obs}, Reward: {reward}, Done: {done}, Truncate: {truncate}, Info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 36.3     |\n",
      "|    ep_rew_mean     | -686     |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x13a32773a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"disentangler_ppo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Disentangler(n_qubits=N_QUBITS,depth=DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 13\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     15\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\gunsu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gunsu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "env.training = False\n",
    "model = PPO.load(\"disentangler_ppo\", env=env)\n",
    "\n",
    "num_episodes = 10\n",
    "total_rewards = []\n",
    "\n",
    "for _ in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode Reward: {episode_reward}\")\n",
    "\n",
    "avg_reward = np.mean(total_rewards)\n",
    "print(f\"Average Reward: {avg_reward}\")\n",
    "plt.plot(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
