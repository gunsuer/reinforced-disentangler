{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cliff2():\n",
    "    \"\"\"\n",
    "    Random 2-qubit Clifford circuit.\n",
    "\n",
    "    Arguments:\n",
    "        -nodes (np.ndarray): \n",
    "    \n",
    "    Returns:\n",
    "        -null\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = np.random.randint(2, size=(2, 10))\n",
    "    \n",
    "    return qml.matrix(qml.RandomLayers(weights=weights,wires=[0,1])).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomLayers(N_QUBITS, DEPTH):\n",
    "    \"\"\"\n",
    "    Generates brick wall pattern of random 2 qubit Clifford gates\n",
    "\n",
    "    Arguments:\n",
    "        -N_QUBITS (int): Number of qubits\n",
    "        -DEPTH (int): Depth of the circuit\n",
    "\n",
    "    Returns:\n",
    "        -random_layers (np.ndarray): Array of 4x4 unitaries (N_QUBITS, DEPTH, 4, 4)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    random_layers = []\n",
    "    for t in range(DEPTH):\n",
    "        layer = []\n",
    "        for x in range(0,N_QUBITS,2):\n",
    "                layer.append(Cliff2())\n",
    "        random_layers.append(layer)\n",
    "\n",
    "    return random_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUBITS = 2*3\n",
    "DEPTH = 2\n",
    "\n",
    "# random_layers = []\n",
    "# # for t in range(DEPTH):\n",
    "# #         layer = []\n",
    "# #         for x in range(0,N_QUBITS,2):\n",
    "# #                 layer.append(Cliff2())\n",
    "# #         random_layers.append(layer)\n",
    "\n",
    "random_layers = RandomLayers(N_QUBITS,DEPTH)\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=N_QUBITS)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def circuit(theta):\n",
    "    \"\"\"\n",
    "    Quantum circuit with random entangling Clifford layers and disentangling layers.\n",
    "    \n",
    "    Arguments:\n",
    "        -theta (np.ndarray): Binary matrix representing the positions of projections. (N_QUBITS, DEPTH)\n",
    "    \n",
    "    Returns:\n",
    "        -Average Von Neumann entropy (float32): Average of 2-qubit Von Neumann entropies over all neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    theta = theta.T\n",
    "    DEPTH,N_QUBITS = np.shape(theta)\n",
    "\n",
    "    for t in range(DEPTH):\n",
    "        layer = random_layers[t]\n",
    "        if t%2==0:\n",
    "            for x in range(0,N_QUBITS,2):\n",
    "                brick = layer[int(x/2)]\n",
    "                qml.QubitUnitary(brick,wires=[x,x+1])\n",
    "        elif t%2==1:\n",
    "            for x in range(1,N_QUBITS-2,2):\n",
    "                brick = layer[int((x-1)/2)]\n",
    "                qml.QubitUnitary(brick,wires=[x,x+1])\n",
    "            brick = layer[-1]\n",
    "            qml.QubitUnitary(brick,wires=[N_QUBITS-1,0])\n",
    "            \n",
    "        projections = theta[t]\n",
    "        for x in range(N_QUBITS):\n",
    "            if projections[x]==1:\n",
    "                qml.Projector(state=[0],wires=[x])\n",
    "            \n",
    "    entropies = []\n",
    "    for x in range(N_QUBITS-1):\n",
    "        entropies.append(qml.vn_entropy(wires=[x,x+1]))\n",
    "    entropies.append(qml.vn_entropy(wires=[N_QUBITS-1,0]))\n",
    "        \n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10857812+0.16546573j,  0.18615456+0.61394576j,\n",
       "        -0.43227776+0.44462681j,  0.28318902+0.29068174j],\n",
       "       [-0.27874883-0.29139345j,  0.54151985-0.38641197j,\n",
       "        -0.33228934-0.34247542j,  0.2258591 +0.34075363j],\n",
       "       [-0.13879178-0.50696963j, -0.05325489-0.2053904j ,\n",
       "         0.04336174+0.61838595j, -0.46627099+0.27750185j],\n",
       "       [ 0.246998  -0.67891932j, -0.29368105+0.12078603j,\n",
       "         0.05769507+0.01127702j,  0.60205322-0.10629554j]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_layers[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5087581128692267, 1.1102230246251564e-16, 1.1102230246251564e-16, 1.1102230246251564e-16, 0.5087581128692285, 1.4701977598569925e-15]\n",
      "<class 'list'>\n",
      "0: ─╭U(M0)─────────╭U(M5)─────────┤ ╭vnentropy                                            \n",
      "1: ─╰U(M0)─╭U(M3)──│───────|0⟩⟨0|─┤ ╰vnentropy ╭vnentropy                                 \n",
      "2: ─╭U(M1)─╰U(M3)──│───────|0⟩⟨0|─┤            ╰vnentropy ╭vnentropy                      \n",
      "3: ─╰U(M1)─╭U(M4)──│───────|0⟩⟨0|─┤                       ╰vnentropy ╭vnentropy           \n",
      "4: ─╭U(M2)─╰U(M4)──│──────────────┤                                  ╰vnentropy ╭vnentropy\n",
      "5: ─╰U(M2)──|0⟩⟨0|─╰U(M5)─────────┤                                             ╰vnentropy\n",
      "\n",
      "  ╭vnentropy\n",
      "  │         \n",
      "  │         \n",
      "  │         \n",
      "  │         \n",
      "  ╰vnentropy\n",
      "\n",
      "M0 = \n",
      "[[ 0.15723302-0.4258975j   0.        +0.j         -0.80175132+0.38869516j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.81143708+0.52527811j  0.        +0.j\n",
      "  -0.11064153+0.23110871j]\n",
      " [-0.73887512-0.49794844j  0.        +0.j         -0.09555685-0.44382405j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.14214835+0.21318216j  0.        +0.j\n",
      "   0.72918937-0.63453139j]]\n",
      "M1 = \n",
      "[[-0.10857812+0.16546573j  0.18615456+0.61394576j -0.43227776+0.44462681j\n",
      "   0.28318902+0.29068174j]\n",
      " [-0.27874883-0.29139345j  0.54151985-0.38641197j -0.33228934-0.34247542j\n",
      "   0.2258591 +0.34075363j]\n",
      " [-0.13879178-0.50696963j -0.05325489-0.2053904j   0.04336174+0.61838595j\n",
      "  -0.46627099+0.27750185j]\n",
      " [ 0.246998  -0.67891932j -0.29368105+0.12078603j  0.05769507+0.01127702j\n",
      "   0.60205322-0.10629554j]]\n",
      "M2 = \n",
      "[[ 0.36602058-0.07987514j -0.18514731-0.42034877j -0.77043736-0.19949589j\n",
      "  -0.1089851 -0.05853369j]\n",
      " [-0.18514731-0.42034877j  0.1737851 -0.15788829j -0.1089851 -0.05853369j\n",
      "   0.84560546+0.05953883j]\n",
      " [ 0.03042888-0.75608878j -0.0096305 -0.27565809j  0.36517478-0.20050103j\n",
      "  -0.37144558-0.19949589j]\n",
      " [-0.0096305 -0.27565809j  0.32356718+0.74372143j -0.37144558-0.19949589j\n",
      "  -0.1089851 -0.27650389j]]\n",
      "M3 = \n",
      "[[-0.05450676-0.09272624j  0.31069644+0.52866665j -0.56872602-0.22925754j\n",
      "  -0.22925754-0.42876896j]\n",
      " [ 0.56872602-0.3096913j   0.16973424-0.56872602j -0.31069644-0.31069644j\n",
      "   0.16346344-0.09272624j]\n",
      " [ 0.04584194-0.38531042j -0.04584194+0.09411512j  0.23114611-0.42846625j\n",
      "  -0.64643645+0.42846625j]\n",
      " [-0.02504357-0.64643645j  0.44911632+0.24301377j  0.38531042+0.21319278j\n",
      "   0.35314984-0.04584194j]]\n",
      "M4 = \n",
      "[[-0.03745339-0.21949976j -0.58068101+0.28356222j -0.64488823+0.07837765j\n",
      "  -0.31015336+0.12083105j]\n",
      " [-0.62283169-0.04190012j -0.03970292-0.37625506j -0.35230404+0.12083105j\n",
      "   0.56773192-0.07837765j]\n",
      " [-0.01380616-0.72738628j  0.01990968-0.05604758j  0.20507104+0.33880126j\n",
      "  -0.12439817-0.54301523j]\n",
      " [-0.12904061-0.12169338j -0.25884554+0.60722245j  0.2335291 -0.47736943j\n",
      "   0.45011042-0.21863743j]]\n",
      "M5 = \n",
      "[[-0.61700168-0.46091401j -0.22927307+0.01625886j -0.41968154-0.02976165j\n",
      "  -0.36923013-0.20171134j]\n",
      " [-0.33706955-0.25179847j  0.41968154-0.02976165j -0.22927307-0.01625886j\n",
      "   0.67587122+0.36923013j]\n",
      " [-0.41968154-0.02976165j  0.33706955-0.25179847j  0.67587122-0.36923013j\n",
      "  -0.22927307+0.01625886j]\n",
      " [-0.22927307-0.01625886j -0.61700168+0.46091401j  0.36923013-0.20171134j\n",
      "   0.41968154-0.02976165j]]\n"
     ]
    }
   ],
   "source": [
    "theta = np.random.randint(2, size=(N_QUBITS,DEPTH))\n",
    "print(circuit(theta))\n",
    "print(type(circuit(theta)))\n",
    "drawer = qml.draw(circuit)\n",
    "\n",
    "print(drawer(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disentangler(gym.Env):\n",
    "    \"\"\"\n",
    "    Reinforcement learning environment for the disentangler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits, depth):\n",
    "        super(Disentangler, self).__init__()\n",
    "        \n",
    "        self.N_QUBITS = n_qubits\n",
    "        self.DEPTH = depth\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(self.N_QUBITS * self.DEPTH)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.N_QUBITS, self.DEPTH), dtype=np.int8)\n",
    "        self.state = np.zeros((self.N_QUBITS, self.DEPTH), dtype=np.int8)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Initialize reward and done\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncate = False\n",
    "\n",
    "        # Apply the action\n",
    "        h = np.zeros(self.N_QUBITS * self.DEPTH, dtype=np.int8)\n",
    "        h[action] = 1\n",
    "        h = h.reshape((self.N_QUBITS, self.DEPTH))\n",
    "        self.state = (self.state + h) % 2\n",
    "\n",
    "        # Calculate entropy (assumes circuit is a predefined function)\n",
    "        entropies = circuit(self.state)\n",
    "        entropy = np.mean(entropies)\n",
    "\n",
    "        # Check if the state is trivial\n",
    "        trivial1 = (np.sum(self.state[:, -1]) == self.N_QUBITS)\n",
    "        trivial2 = (np.sum(self.state[:, -1]) == self.N_QUBITS - 1)\n",
    "        trivial = trivial1 or trivial2\n",
    "\n",
    "        # Determine reward and done conditions\n",
    "        if entropy < 1e-17:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        elif trivial:\n",
    "            reward = -1000\n",
    "            truncate = True\n",
    "        \n",
    "        # Return the state, reward, done flag, and info\n",
    "        info = {}\n",
    "        return self.state, reward, done, truncate, info\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # Seed the random number generator if a seed is provided\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Reset the state to an all-zero matrix\n",
    "        self.state = np.zeros((self.N_QUBITS, self.DEPTH), dtype=np.int8)\n",
    "\n",
    "        info = {}\n",
    "        return self.state, info\n",
    "    \n",
    "    def render(self):\n",
    "        print()\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Implement any cleanup\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Disentangler(n_qubits=N_QUBITS,depth=DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: \n",
      " (tensor([[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]], dtype=int8, requires_grad=True), {})\n",
      "Observation: \n",
      " [[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]], Reward: 0, Done: False, Truncate: False, Info: {}\n"
     ]
    }
   ],
   "source": [
    "env = Disentangler(n_qubits=N_QUBITS, depth=DEPTH)\n",
    "obs = env.reset()\n",
    "print(f\"Initial Observation: \\n {obs}\")\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, reward, done, truncate, info = env.step(action)\n",
    "print(f\"Observation: \\n {obs}, Reward: {reward}, Done: {done}, Truncate: {truncate}, Info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0],\n",
       "         [0, 0]], dtype=int8, requires_grad=True),\n",
       " {})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.6     |\n",
      "|    ep_rew_mean     | -834     |\n",
      "| time/              |          |\n",
      "|    fps             | 106      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1c10d72a640>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m obs \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     action, _states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     obs, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m      5\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\gunsu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gunsu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncate, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gunsu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gunsu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "model.predict(obs,deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
