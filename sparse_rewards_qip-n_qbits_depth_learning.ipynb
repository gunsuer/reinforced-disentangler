{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyclifford as pc\n",
    "import gymnasium as gym\n",
    "import stable_baselines3\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclifford as pc\n",
    "from pyclifford.utils import mask, condense, pauli_diagonalize1,stabilizer_measure\n",
    "from pyclifford.paulialg import Pauli, pauli, PauliMonomial, pauli_zero\n",
    "from pyclifford.stabilizer import (StabilizerState, CliffordMap,\n",
    "    zero_state, identity_map, clifford_rotation_map, random_clifford_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layers(N_QUBITS,HALF_DEPTH):\n",
    "    random_layers=[]\n",
    "    for i in range (int(HALF_DEPTH)):\n",
    "        random_layer=[]\n",
    "        if i%2==0:\n",
    "            for j in range (int(np.floor(N_QUBITS/2))):\n",
    "                gate=pc.CliffordGate(j*2,j*2+1)\n",
    "                gate.set_forward_map(pc.random_clifford_map(2))\n",
    "                random_layer.append(gate)\n",
    "            random_layers.append(random_layer)\n",
    "        elif i%2==1:\n",
    "            for j in range (int(np.floor(N_QUBITS/2))):\n",
    "                gate=pc.CliffordGate(j*2+1,(j*2+2)%N_QUBITS)\n",
    "                gate.set_forward_map(pc.random_clifford_map(2))\n",
    "                random_layer.append(gate)\n",
    "            random_layers.append(random_layer)\n",
    "    return random_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_layers(N_QUBITS,HALF_DEPTH,theta):\n",
    "    measure_layers=[]\n",
    "    for i in range (int(HALF_DEPTH)):\n",
    "        measure_layer=[]\n",
    "        for j in range (int(N_QUBITS)):\n",
    "            if theta[-i + HALF_DEPTH - 1][j]==1:\n",
    "                measure_layer.append(j)\n",
    "        measure_layers.append(measure_layer)\n",
    "    return measure_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_circuit(N_QUBITS,HALF_DEPTH,random_layers,measure_layers):\n",
    "    circ = pc.circuit.Circuit(N_QUBITS)\n",
    "    for i in range(int(HALF_DEPTH)):\n",
    "        for j in range(int(np.floor(N_QUBITS/2))):\n",
    "            circ.take(random_layers[i][j])\n",
    "        if measure_layers[i]!=[]:\n",
    "            qubits=tuple(measure_layers[i])\n",
    "            circ.measure(*qubits)\n",
    "            \n",
    "    return circ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaged_EE(state_final):\n",
    "    EE_positions=[]\n",
    "    for i in range(0,state_final.N):\n",
    "        EE_positions.append([int(i),int(i+1)%state_final.N])\n",
    "        \n",
    "    EE_list=[]\n",
    "    for i in range(0,len(EE_positions)):\n",
    "        EE_list.append(state_final.entropy(EE_positions[i]))\n",
    "    \n",
    "    averaged_EE=np.mean(EE_list)\n",
    "        \n",
    "    return averaged_EE #, EE_positions, EE_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalty(x,penalty_slope):\n",
    "    return 2*(0.5-(1/(1+np.exp(-penalty_slope*x))-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Disentangler(gym.Env):\n",
    "    \"\"\"\n",
    "    Reinforcement learning environment for the disentangler.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits, half_depth, positive_reward, penalty_slope):\n",
    "        super(Disentangler, self).__init__()\n",
    "        \n",
    "        self.N_QUBITS = n_qubits\n",
    "        self.HALF_DEPTH = half_depth\n",
    "        self.DEPTH = 2*half_depth\n",
    "        self.positive_reward = positive_reward\n",
    "        self.penalty_slope = penalty_slope\n",
    "        \n",
    "        self.action_space = gym.spaces.Discrete(self.N_QUBITS * self.HALF_DEPTH)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.HALF_DEPTH, self.N_QUBITS), dtype=np.int8)\n",
    "\n",
    "        self.random_layers=random_layers(self.N_QUBITS,self.HALF_DEPTH)\n",
    "        self.theta = np.zeros((self.HALF_DEPTH, self.N_QUBITS), dtype=np.int8)\n",
    "        self.measure_layers=measure_layers(self.N_QUBITS,self.HALF_DEPTH,self.theta)\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Initialize reward and done\n",
    "        reward = 0\n",
    "        done = False\n",
    "        truncate = False\n",
    "\n",
    "        # Apply the action\n",
    "        h = np.zeros(self.N_QUBITS * self.HALF_DEPTH, dtype=np.int8)\n",
    "        h[action] = 1\n",
    "        h = h.reshape((self.HALF_DEPTH, self.N_QUBITS))\n",
    "        self.theta = (self.theta + h) % 2\n",
    "\n",
    "        # Calculate entropy (assumes circuit is a predefined function)\n",
    "        self.measure_layers=measure_layers(self.N_QUBITS, self.HALF_DEPTH, self.theta)\n",
    "        circ = create_circuit(self.N_QUBITS, self.HALF_DEPTH, self.random_layers, self.measure_layers)\n",
    "\n",
    "        state_initial = pc.stabilizer.zero_state(self.N_QUBITS)\n",
    "        state_final = circ.forward(state_initial)\n",
    "\n",
    "        entropy = averaged_EE(state_final)\n",
    "\n",
    "        \n",
    "        if entropy == 0:\n",
    "            m_per_layer = [np.sum(layer) for layer in self.theta]\n",
    "            cost = [m_per_layer[i]*penalty(i,self.penalty_slope) for i in range(self.HALF_DEPTH)]\n",
    "            reward = self.positive_reward - np.sum(cost)#/(self.N_QUBITS*self.DEPTH)\n",
    "            done = True\n",
    "        \n",
    "        # Return the state, reward, done flag, truncate flag, and info\n",
    "        info = {}\n",
    "        return self.theta, reward, done, truncate, info\n",
    "    \n",
    "    def return_entropy(self):\n",
    "        # Calculate entropy (assumes circuit is a predefined function)\n",
    "        self.measure_layers=measure_layers(self.N_QUBITS, self.HALF_DEPTH, self.theta)\n",
    "        circ = create_circuit(self.N_QUBITS, self.HALF_DEPTH, self.random_layers, self.measure_layers)\n",
    "\n",
    "        state_initial = pc.stabilizer.zero_state(self.N_QUBITS)\n",
    "        state_final = circ.forward(state_initial)\n",
    "\n",
    "        entropy = averaged_EE(state_final)\n",
    "        \n",
    "        return entropy\n",
    "        \n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        # Seed the random number generator if a seed is provided\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Reset the state to an all-zero matrix\n",
    "        self.theta = np.zeros((self.HALF_DEPTH, self.N_QUBITS), dtype=np.int8)\n",
    "\n",
    "        # Reset the random layers to another random layers\n",
    "        self.random_layers=random_layers(self.N_QUBITS,self.HALF_DEPTH)\n",
    "        \n",
    "        info = {}\n",
    "        return self.theta, info\n",
    "    \n",
    "    def render(self):\n",
    "        print()\n",
    "\n",
    "    def close(self):\n",
    "        # Optional: Implement any cleanup\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo parameters\n",
    "ts = 0.5*1e6\n",
    "lr = 0.001\n",
    "ec = 0.01\n",
    "#ec_list=[0.01,0.1,0.2,0.3,0.4,0.5,1,2,3,4,5]\n",
    "\n",
    "# circuit parameters\n",
    "#N_QUBITS=10\n",
    "N_QUBITS_list=[2,3,4,5,6,7,8,9,10]#,15,20,25,30]\n",
    "\n",
    "#HALF_DEPTH=2\n",
    "#DEPTH=int(2*HALF_DEPTH)\n",
    "HALF_DEPTH_list=[11,12,13,14,16,17,18,19]\n",
    "DEPTH_list=[int(2*HALF_DEPTH_list[i]) for i in range(np.size(HALF_DEPTH_list))]\n",
    "\n",
    "positive_reward = 50\n",
    "penalty_slope = 0#[0,0.5,1,2,3,4,5]\n",
    "\n",
    "#num_full_learning = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_QUBITS = 2\n",
      "HALF_DEPTH = 11\n",
      "learning done\n",
      "HALF_DEPTH = 12\n",
      "learning done\n",
      "HALF_DEPTH = 13\n",
      "learning done\n",
      "HALF_DEPTH = 14\n",
      "learning done\n",
      "HALF_DEPTH = 16\n",
      "learning done\n",
      "HALF_DEPTH = 17\n",
      "learning done\n",
      "HALF_DEPTH = 18\n",
      "learning done\n",
      "HALF_DEPTH = 19\n",
      "learning done\n",
      "N_QUBITS = 3\n",
      "HALF_DEPTH = 11\n",
      "learning done\n",
      "HALF_DEPTH = 12\n",
      "learning done\n",
      "HALF_DEPTH = 13\n",
      "learning done\n",
      "HALF_DEPTH = 14\n",
      "learning done\n",
      "HALF_DEPTH = 16\n",
      "learning done\n",
      "HALF_DEPTH = 17\n",
      "learning done\n",
      "HALF_DEPTH = 18\n"
     ]
    }
   ],
   "source": [
    "#This part is to check the stability of learning\n",
    "model_set=[[] for i in range(np.size(HALF_DEPTH_list))]\n",
    "\n",
    "for i in range(np.size(N_QUBITS_list)):\n",
    "    N_QUBITS = N_QUBITS_list[i]\n",
    "    print(\"N_QUBITS\",'=',N_QUBITS)\n",
    "    for j in range(np.size(HALF_DEPTH_list)):\n",
    "        HALF_DEPTH = HALF_DEPTH_list[j]\n",
    "        print(\"HALF_DEPTH\",'=',HALF_DEPTH)\n",
    "        env = Disentangler(N_QUBITS,HALF_DEPTH,positive_reward,penalty_slope)\n",
    "\n",
    "        env.reset()\n",
    "        model = PPO('MlpPolicy', env, verbose=0, tensorboard_log=\"./tensorboard_files\", learning_rate=lr, ent_coef=ec)\n",
    "\n",
    "        model.learn(total_timesteps=ts, tb_log_name=\"{N_QUBITS}x{HALF_DEPTH}_tb_{ts}_pr_{positive_reward}_ps_{penalty_slope}\".format(N_QUBITS=N_QUBITS, HALF_DEPTH=HALF_DEPTH, ts=ts, positive_reward=positive_reward, penalty_slope=penalty_slope))\n",
    "        model_path = \"./models/{N_QUBITS}x{HALF_DEPTH}_tb_{ts}_pr_{positive_reward}_ps_{penalty_slope}\".format(N_QUBITS=N_QUBITS, HALF_DEPTH=HALF_DEPTH, ts=ts, positive_reward=positive_reward, penalty_slope=penalty_slope)\n",
    "        model_set[i].append(model_path)\n",
    "        model.save(model_path)\n",
    "        print(\"learning done\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-sparse rewards\n",
    "#normalized reward?\n",
    "#area-law, volume-law"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
